{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "from bcblib.tools.general_utils import open_json, save_json\n",
    "from lesseg_unet.utils import get_sex_int, get_age_int\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T10:30:24.288266684Z",
     "start_time": "2023-07-12T10:30:24.260542885Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from lesseg_unet.utils import get_perf_seg_dict, get_perf_seg_dict_from_folders\n",
    "\"\"\"\n",
    "First we need to create a dictionary with the segmentation files and the performance\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "INPUTS\n",
    "\"\"\"\n",
    "seg_folder = '/media/chrisfoulon/HDD2/final_training_set/abnormal_5fold_segmentation/'\n",
    "output_folder = '/media/chrisfoulon/HDD2/final_training_set/perf_analyses_abnormal_5fold_segmentation/'\n",
    "# seg_folder = '/media/chrisfoulon/HDD2/final_training_set/controls_5fold_segmentation/'\n",
    "# output_folder = '/media/chrisfoulon/HDD2/final_training_set/perf_analyses_controls_5fold_segmentation/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T11:57:42.448489707Z",
     "start_time": "2023-07-12T11:57:42.406416581Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-12T11:02:11.062475507Z",
     "start_time": "2023-07-12T10:53:06.372669444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process from folders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 713/713 [00:53<00:00, 13.40it/s]\n",
      "100%|██████████| 713/713 [00:53<00:00, 13.30it/s]\n",
      "100%|██████████| 713/713 [00:53<00:00, 13.28it/s]\n",
      "100%|██████████| 712/712 [00:53<00:00, 13.22it/s]\n",
      "100%|██████████| 712/712 [00:53<00:00, 13.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.2.840.846310145.12.1.1.60692052_20190315163801_201__pref__': {'Unnamed: 0': 70,\n",
      "                                                                  'b1000': '/media/chrisfoulon/HDD2/final_training_set/cleaned_abnormal_images/non_linear_co-rigid_rigid_geomean_denoise_1.2.840.846310145.12.1.1.60692052_20190315163801_201__pref___bval1000.nii.gz',\n",
      "                                                                  'core_filename': 'non_linear_co-rigid_rigid_geomean_denoise_1.2.840.846310145.12.1.1.60692052_20190315163801_201__pref___bval1000_v174v',\n",
      "                                                                  'dice_metric': 0.9855907559394836,\n",
      "                                                                  'distance': 0.0,\n",
      "                                                                  'label': '/media/chrisfoulon/HDD2/final_training_set/controls_5fold_segmentation/fold_1/val_images/R_Thalamoperforators/label_non_linear_co-rigid_rigid_geomean_denoise_1.2.840.846310145.12.1.1.60692052_20190315163801_201__pref___bval1000_v174v_71.nii.gz',\n",
      "                                                                  'segmentation': '/media/chrisfoulon/HDD2/final_training_set/controls_5fold_segmentation/fold_1/val_images/R_Thalamoperforators/output_non_linear_co-rigid_rigid_geomean_denoise_1.2.840.846310145.12.1.1.60692052_20190315163801_201__pref___bval1000_v174v_71.nii.gz',\n",
      "                                                                  'volume': 174}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3563/3563 [04:36<00:00, 12.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3563\n",
      "{'wodctH25_b1000_clmp_b1000_40353637_M_19530720_20081222_SIEMENS_Avanto_15_MRHead_ep2d_diff_3scan_ipat_ep_b1000t_1': {'Unnamed: 0': 239,\n",
      "                                                                                                                      'b1000': '/media/chrisfoulon/HDD2/final_training_set/cleaned_abnormal_images/wodctH25_b1000_clmp_b1000_40353637_M_19530720_20081222_SIEMENS_Avanto_15_MRHead_ep2d_diff_3scan_ipat_ep_b1000t_1.nii.gz',\n",
      "                                                                                                                      'core_filename': 'wodctH25_b1000_clmp_b1000_40353637_M_19530720_20081222_SIEMENS_Avanto_15_MRHead_ep2d_diff_3scan_ipat_ep_b1000t_1_v230v',\n",
      "                                                                                                                      'dice_metric': 0.7714987993240356,\n",
      "                                                                                                                      'distance': 33.12099032335839,\n",
      "                                                                                                                      'label': '/media/chrisfoulon/HDD2/final_training_set/controls_5fold_segmentation/fold_0/val_images/L_Precentral/label_wodctH25_b1000_clmp_b1000_40353637_M_19530720_20081222_SIEMENS_Avanto_15_MRHead_ep2d_diff_3scan_ipat_ep_b1000t_1_v230v_240.nii.gz',\n",
      "                                                                                                                      'segmentation': '/media/chrisfoulon/HDD2/final_training_set/controls_5fold_segmentation/fold_0/val_images/L_Precentral/output_wodctH25_b1000_clmp_b1000_40353637_M_19530720_20081222_SIEMENS_Avanto_15_MRHead_ep2d_diff_3scan_ipat_ep_b1000t_1_v230v_240.nii.gz',\n",
      "                                                                                                                      'volume': 230}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "spm_seg_folder = Path(output_folder, 'spm_segmentation')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(spm_seg_folder, exist_ok=True)\n",
    "\n",
    "keys_struct = open_json('/media/chrisfoulon/HDD2/final_training_set/cleaned_abnormal_b1000_info_dict.json')\n",
    "\n",
    "if Path(seg_folder, '__output_image_volumes.json').exists():\n",
    "    seg_dict = get_perf_seg_dict(seg_folder, keys_struct=keys_struct, relative_output_paths=False)\n",
    "else:\n",
    "    print('process from folders')\n",
    "    seg_dict = get_perf_seg_dict_from_folders(seg_folder, keys_struct=keys_struct, relative_output_paths=False)\n",
    "# print first value of the dict\n",
    "pprint({k: seg_dict[k] for k in random.sample(seg_dict.keys(), 1)})\n",
    "# Then we need to copy the segmentation files to a separate folder for SPM ...\n",
    "output_masks_folder = Path(output_folder, 'output_masks/')\n",
    "output_labels_folder = Path(output_folder, 'output_labels/')\n",
    "os.makedirs(output_masks_folder, exist_ok=True)\n",
    "os.makedirs(output_labels_folder, exist_ok=True)\n",
    "spm_seg_dict = {}\n",
    "for k in tqdm(seg_dict):\n",
    "    shutil.copy(seg_dict[k]['segmentation'], output_masks_folder)\n",
    "    shutil.copy(seg_dict[k]['label'], output_labels_folder)\n",
    "    spm_seg_dict[k] = deepcopy(seg_dict[k])\n",
    "    spm_seg_dict[k]['segmentation'] = str(Path(spm_seg_folder, Path(seg_dict[k]['segmentation']).name.split('.gz')[0]))\n",
    "    nib.save(nib.load(seg_dict[k]['segmentation']), spm_seg_dict[k]['segmentation'])\n",
    "print(len(seg_dict))\n",
    "pprint({k: seg_dict[k] for k in random.sample(seg_dict.keys(), 1)})\n",
    "\n",
    "save_json(Path(seg_folder, 'perf_seg_dict.json'), seg_dict)\n",
    "save_json(Path(seg_folder, 'spm_perf_seg_dict.json'), spm_seg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Then we need to create a dataframe with the performance metrics\n",
    "\"\"\"\n",
    "columns=['segmentation', 'dice_metric', 'distance', 'volume', 'lesion_cluster', 'PatientAge', 'PatientSex']\n",
    "b1000_info_dict = open_json('/media/chrisfoulon/HDD2/final_training_set/cleaned_abnormal_b1000_info_dict.json')\n",
    "\n",
    "\n",
    "# dict_for_df = {}\n",
    "# for k in seg_dict:\n",
    "#     for col in columns:\n",
    "#         if col in seg_dict:\n",
    "#             dict_for_df[k][col] = seg_dict[k][col]\n",
    "#         else:\n",
    "#             dict_for_df[k][col] = b1000_info_dict[k][col]\n",
    "#\n",
    "# pprint({k: dict_for_df[k] for k in random.sample(dict_for_df.keys(), 1)})\n",
    "# new_df = pd.DataFrame.from_records(dict_for_df).T\n",
    "# new_df.to_csv(Path(seg_folder, 'segmentation_perf_df.csv'), columns=columns)\n",
    "\n",
    "# create a function to do that\n",
    "def get_df_from_dict(seg_dict, columns):\n",
    "    dict_for_df = {}\n",
    "    for k in seg_dict:\n",
    "        dict_for_df[k] = {}\n",
    "        for col in columns:\n",
    "            if col in seg_dict[k]:\n",
    "                if col == 'PatientAge':\n",
    "                    dict_for_df[k][col] = get_age_int(seg_dict[k][col])\n",
    "                elif col == 'PatientSex':\n",
    "                    dict_for_df[k][col] = get_sex_int(seg_dict[k][col])\n",
    "                else:\n",
    "                    dict_for_df[k][col] = seg_dict[k][col]\n",
    "            elif col == 'lesion_cluster':\n",
    "                dict_for_df[k][col] = Path(seg_dict[k]['segmentation']).parent.name\n",
    "            else:\n",
    "                if col == 'PatientAge':\n",
    "                    dict_for_df[k][col] = get_age_int(b1000_info_dict[k][col])\n",
    "                elif col == 'PatientSex':\n",
    "                    dict_for_df[k][col] = get_sex_int(b1000_info_dict[k][col])\n",
    "                else:\n",
    "                    dict_for_df[k][col] = b1000_info_dict[k][col]\n",
    "    new_df = pd.DataFrame.from_records(dict_for_df).T\n",
    "    return new_df\n",
    "\n",
    "# use it on seg_dict and spm_seg_dict\n",
    "new_df = get_df_from_dict(seg_dict, columns)\n",
    "new_df.to_csv(Path(output_folder, 'segmentation_perf_df.csv'), columns=columns)\n",
    "new_df = get_df_from_dict(spm_seg_dict, columns)\n",
    "new_df.to_csv(Path(output_folder, 'spm_segmentation_perf_df.csv'), columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T11:02:11.467181434Z",
     "start_time": "2023-07-12T11:02:11.069424756Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Then we need to zscore the columns\n",
    "\"\"\"\n",
    "zcores_columns = ['dice_metric', 'distance', 'volume', 'PatientAge', 'PatientSex']\n",
    "for col in zcores_columns:\n",
    "    # pd.to_numeric(new_df[col])\n",
    "    new_df[col] = new_df[col].infer_objects()\n",
    "#     print(col)\n",
    "#     print(new_df[col].dtype)\n",
    "#     new_df[col] = new_df[col].apply(stats.zscore)\n",
    "normalized_df = new_df[zcores_columns]\n",
    "normalized_df=(normalized_df-normalized_df.mean())/normalized_df.std()\n",
    "normalized_df=(normalized_df-normalized_df.min())/(normalized_df.max()-normalized_df.min())\n",
    "new_df[zcores_columns] = normalized_df[zcores_columns]\n",
    "new_df.to_csv(Path(seg_folder, 'segmentation_perf_df_zscored.csv'), columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T11:02:11.564137453Z",
     "start_time": "2023-07-12T11:02:11.467393651Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3563/3563 [03:59<00:00, 14.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from nilearn.image import smooth_img\n",
    "# smooth all the images from the segmentation dict with key 'segmentation' and save them in '/media/chrisfoulon/HDD2/final_training_set/abnormal_5fold_segmentation_masks_smoothed_8'\n",
    "smooth_out_folder = output_folder + 'masks_smoothed_8'\n",
    "os.makedirs(smooth_out_folder, exist_ok=True)\n",
    "spm_smoothed_seg_dict = {}\n",
    "for k in tqdm(spm_seg_dict):\n",
    "    smoothed_img = smooth_img(spm_seg_dict[k]['segmentation'], 8)\n",
    "    spm_smoothed_seg_dict[k] = deepcopy(spm_seg_dict[k])\n",
    "    spm_smoothed_seg_dict[k]['segmentation'] = str(Path(smooth_out_folder, Path(spm_seg_dict[k]['segmentation']).name))\n",
    "    nib.save(smoothed_img, spm_smoothed_seg_dict[k]['segmentation'])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T11:06:11.169330134Z",
     "start_time": "2023-07-12T11:02:11.568960133Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "new_df = get_df_from_dict(spm_smoothed_seg_dict, columns)\n",
    "new_df.to_csv(Path(output_folder, 'spm_smoothed_segmentation_perf_df.csv'), columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T11:58:04.182620335Z",
     "start_time": "2023-07-12T11:58:03.990497196Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
