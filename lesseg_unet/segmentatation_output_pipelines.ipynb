{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "from bcblib.tools.general_utils import open_json, save_json\n",
    "from lesseg_unet.utils import get_sex_int, get_age_int\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T14:12:03.059434269Z",
     "start_time": "2023-07-19T14:12:00.316082473Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from lesseg_unet.utils import get_perf_seg_dict, get_perf_seg_dict_from_folders\n",
    "\"\"\"\n",
    "First we need to create a dictionary with the segmentation files and the performance\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "INPUTS\n",
    "\"\"\"\n",
    "# seg_folder = '/media/chrisfoulon/HDD2/final_training_set/abnormal_5fold_segmentation/'\n",
    "# output_folder = '/media/chrisfoulon/HDD2/final_training_set/perf_analyses_abnormal_5fold_segmentation/'\n",
    "seg_folder = '/media/chrisfoulon/HDD2/final_training_set/controls_trained_5fold_segmentation/'\n",
    "output_folder = '/media/chrisfoulon/HDD2/final_training_set/perf_analyses_controls_5fold_segmentation/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T17:14:42.054664169Z",
     "start_time": "2023-07-14T17:14:42.025339967Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-14T17:22:25.846349920Z",
     "start_time": "2023-07-14T17:14:43.025998698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process from folders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 713/713 [00:52<00:00, 13.61it/s]\n",
      "100%|██████████| 713/713 [00:52<00:00, 13.56it/s]\n",
      "100%|██████████| 713/713 [00:52<00:00, 13.52it/s]\n",
      "100%|██████████| 712/712 [00:52<00:00, 13.54it/s]\n",
      "100%|██████████| 712/712 [00:52<00:00, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DTI_P6d_noniso_SENSE_20161027150912_201__pref__': {'Unnamed: 0': 410,\n",
      "                                                     'b1000': '/media/chrisfoulon/HDD2/final_training_set/cleaned_abnormal_images/non_linear_co-rigid_rigid_geomean_denoise_DTI_P6d_noniso_SENSE_20161027150912_201__pref___bval1000.nii.gz',\n",
      "                                                     'core_filename': 'non_linear_co-rigid_rigid_geomean_denoise_DTI_P6d_noniso_SENSE_20161027150912_201__pref___bval1000_v32v',\n",
      "                                                     'dice_metric': 0.8148148059844971,\n",
      "                                                     'distance': 1.0,\n",
      "                                                     'label': '/media/chrisfoulon/HDD2/final_training_set/controls_trained_5fold_segmentation/fold_4/val_images/R_Cerebellar/label_non_linear_co-rigid_rigid_geomean_denoise_DTI_P6d_noniso_SENSE_20161027150912_201__pref___bval1000_v32v_411.nii.gz',\n",
      "                                                     'segmentation': '/media/chrisfoulon/HDD2/final_training_set/controls_trained_5fold_segmentation/fold_4/val_images/R_Cerebellar/output_non_linear_co-rigid_rigid_geomean_denoise_DTI_P6d_noniso_SENSE_20161027150912_201__pref___bval1000_v32v_411.nii.gz',\n",
      "                                                     'volume': 32}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3563/3563 [03:19<00:00, 17.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3563\n",
      "{'1.2.840.846310145.12.1.1.60054867_20180503152526_201__pref__': {'Unnamed: 0': 532,\n",
      "                                                                  'b1000': '/media/chrisfoulon/HDD2/final_training_set/cleaned_abnormal_images/non_linear_co-rigid_rigid_geomean_denoise_1.2.840.846310145.12.1.1.60054867_20180503152526_201__pref___bval1000.nii.gz',\n",
      "                                                                  'core_filename': 'non_linear_co-rigid_rigid_geomean_denoise_1.2.840.846310145.12.1.1.60054867_20180503152526_201__pref___bval1000_v15675v',\n",
      "                                                                  'dice_metric': 0.9085841178894044,\n",
      "                                                                  'distance': 1.4142135623730951,\n",
      "                                                                  'label': '/media/chrisfoulon/HDD2/final_training_set/controls_trained_5fold_segmentation/fold_1/val_images/L_Anterior_MCA/label_non_linear_co-rigid_rigid_geomean_denoise_1.2.840.846310145.12.1.1.60054867_20180503152526_201__pref___bval1000_v15675v_533.nii.gz',\n",
      "                                                                  'segmentation': '/media/chrisfoulon/HDD2/final_training_set/controls_trained_5fold_segmentation/fold_1/val_images/L_Anterior_MCA/output_non_linear_co-rigid_rigid_geomean_denoise_1.2.840.846310145.12.1.1.60054867_20180503152526_201__pref___bval1000_v15675v_533.nii.gz',\n",
      "                                                                  'volume': 15675}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "spm_seg_folder = Path(output_folder, 'spm_segmentation')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(spm_seg_folder, exist_ok=True)\n",
    "\n",
    "keys_struct = open_json('/media/chrisfoulon/HDD2/final_training_set/cleaned_abnormal_b1000_info_dict.json')\n",
    "\n",
    "if Path(seg_folder, '__output_image_volumes.json').exists():\n",
    "    seg_dict = get_perf_seg_dict(seg_folder, keys_struct=keys_struct, relative_output_paths=False)\n",
    "else:\n",
    "    print('process from folders')\n",
    "    seg_dict = get_perf_seg_dict_from_folders(seg_folder, keys_struct=keys_struct, relative_output_paths=False)\n",
    "# print first value of the dict\n",
    "pprint({k: seg_dict[k] for k in random.sample(seg_dict.keys(), 1)})\n",
    "# Then we need to copy the segmentation files to a separate folder for SPM ...\n",
    "output_masks_folder = Path(output_folder, 'output_masks/')\n",
    "output_labels_folder = Path(output_folder, 'output_labels/')\n",
    "os.makedirs(output_masks_folder, exist_ok=True)\n",
    "os.makedirs(output_labels_folder, exist_ok=True)\n",
    "spm_seg_dict = {}\n",
    "for k in tqdm(seg_dict):\n",
    "    shutil.copy(seg_dict[k]['segmentation'], output_masks_folder)\n",
    "    shutil.copy(seg_dict[k]['label'], output_labels_folder)\n",
    "    spm_seg_dict[k] = deepcopy(seg_dict[k])\n",
    "    spm_seg_dict[k]['segmentation'] = str(Path(spm_seg_folder, Path(seg_dict[k]['segmentation']).name.split('.gz')[0]))\n",
    "    nib.save(nib.load(seg_dict[k]['segmentation']), spm_seg_dict[k]['segmentation'])\n",
    "print(len(seg_dict))\n",
    "pprint({k: seg_dict[k] for k in random.sample(seg_dict.keys(), 1)})\n",
    "\n",
    "save_json(Path(seg_folder, 'perf_seg_dict.json'), seg_dict)\n",
    "save_json(Path(seg_folder, 'spm_perf_seg_dict.json'), spm_seg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Then we need to create a dataframe with the performance metrics\n",
    "\"\"\"\n",
    "columns=['segmentation', 'dice_metric', 'distance', 'volume', 'lesion_cluster', 'PatientAge', 'PatientSex']\n",
    "b1000_info_dict = open_json('/media/chrisfoulon/HDD2/final_training_set/cleaned_abnormal_b1000_info_dict.json')\n",
    "\n",
    "\n",
    "# dict_for_df = {}\n",
    "# for k in seg_dict:\n",
    "#     for col in columns:\n",
    "#         if col in seg_dict:\n",
    "#             dict_for_df[k][col] = seg_dict[k][col]\n",
    "#         else:\n",
    "#             dict_for_df[k][col] = b1000_info_dict[k][col]\n",
    "#\n",
    "# pprint({k: dict_for_df[k] for k in random.sample(dict_for_df.keys(), 1)})\n",
    "# new_df = pd.DataFrame.from_records(dict_for_df).T\n",
    "# new_df.to_csv(Path(seg_folder, 'segmentation_perf_df.csv'), columns=columns)\n",
    "\n",
    "# create a function to do that\n",
    "def get_df_from_dict(seg_dict, columns):\n",
    "    dict_for_df = {}\n",
    "    for k in seg_dict:\n",
    "        dict_for_df[k] = {}\n",
    "        for col in columns:\n",
    "            if col in seg_dict[k]:\n",
    "                if col == 'PatientAge':\n",
    "                    dict_for_df[k][col] = get_age_int(seg_dict[k][col])\n",
    "                elif col == 'PatientSex':\n",
    "                    dict_for_df[k][col] = get_sex_int(seg_dict[k][col])\n",
    "                else:\n",
    "                    dict_for_df[k][col] = seg_dict[k][col]\n",
    "            elif col == 'lesion_cluster':\n",
    "                dict_for_df[k][col] = Path(seg_dict[k]['segmentation']).parent.name\n",
    "            else:\n",
    "                if col == 'PatientAge':\n",
    "                    dict_for_df[k][col] = get_age_int(b1000_info_dict[k][col])\n",
    "                elif col == 'PatientSex':\n",
    "                    dict_for_df[k][col] = get_sex_int(b1000_info_dict[k][col])\n",
    "                else:\n",
    "                    dict_for_df[k][col] = b1000_info_dict[k][col]\n",
    "    new_df = pd.DataFrame.from_records(dict_for_df).T\n",
    "    return new_df\n",
    "\n",
    "# use it on seg_dict and spm_seg_dict\n",
    "new_df = get_df_from_dict(seg_dict, columns)\n",
    "new_df.to_csv(Path(output_folder, 'segmentation_perf_df.csv'), columns=columns)\n",
    "new_df = get_df_from_dict(spm_seg_dict, columns)\n",
    "new_df.to_csv(Path(output_folder, 'spm_segmentation_perf_df.csv'), columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T17:22:26.250821595Z",
     "start_time": "2023-07-14T17:22:25.857283058Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Then we need to zscore the columns\n",
    "\"\"\"\n",
    "zcores_columns = ['dice_metric', 'distance', 'volume', 'PatientAge', 'PatientSex']\n",
    "for col in zcores_columns:\n",
    "    # pd.to_numeric(new_df[col])\n",
    "    new_df[col] = new_df[col].infer_objects()\n",
    "#     print(col)\n",
    "#     print(new_df[col].dtype)\n",
    "#     new_df[col] = new_df[col].apply(stats.zscore)\n",
    "normalized_df = new_df[zcores_columns]\n",
    "normalized_df=(normalized_df-normalized_df.mean())/normalized_df.std()\n",
    "normalized_df=(normalized_df-normalized_df.min())/(normalized_df.max()-normalized_df.min())\n",
    "new_df[zcores_columns] = normalized_df[zcores_columns]\n",
    "new_df.to_csv(Path(seg_folder, 'segmentation_perf_df_zscored.csv'), columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T17:22:26.341691439Z",
     "start_time": "2023-07-14T17:22:26.253481165Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3563/3563 [04:26<00:00, 13.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from nilearn.image import smooth_img\n",
    "# smooth all the images from the segmentation dict with key 'segmentation' and save them in '/media/chrisfoulon/HDD2/final_training_set/abnormal_5fold_segmentation_masks_smoothed_8'\n",
    "smooth_out_folder = output_folder + 'masks_smoothed_8'\n",
    "os.makedirs(smooth_out_folder, exist_ok=True)\n",
    "spm_smoothed_seg_dict = {}\n",
    "for k in tqdm(spm_seg_dict):\n",
    "    smoothed_img = smooth_img(spm_seg_dict[k]['segmentation'], 8)\n",
    "    spm_smoothed_seg_dict[k] = deepcopy(spm_seg_dict[k])\n",
    "    spm_smoothed_seg_dict[k]['segmentation'] = str(Path(smooth_out_folder, Path(spm_seg_dict[k]['segmentation']).name))\n",
    "    nib.save(smoothed_img, spm_smoothed_seg_dict[k]['segmentation'])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T17:26:53.251930117Z",
     "start_time": "2023-07-14T17:22:26.345046254Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "new_df = get_df_from_dict(spm_smoothed_seg_dict, columns)\n",
    "new_df.to_csv(Path(output_folder, 'spm_smoothed_segmentation_perf_df.csv'), columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T17:26:53.412237958Z",
     "start_time": "2023-07-14T17:26:53.251629046Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3563it [08:44,  6.80it/s]\n",
      "3563it [09:35,  6.19it/s]\n",
      "3563it [49:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from lesseg_unet.utils import weight_lesion_dataset\n",
    "\n",
    "abnormal_df_path = Path('/media/chrisfoulon/HDD2/final_training_set/perf_analyses_abnormal_5fold_segmentation/', 'spm_smoothed_segmentation_perf_df.csv')\n",
    "controls_df_path = Path('/media/chrisfoulon/HDD2/final_training_set/perf_analyses_controls_5fold_segmentation/', 'spm_smoothed_segmentation_perf_df.csv')\n",
    "\n",
    "abnormal_weighted_output_folder = Path('/media/chrisfoulon/HDD2/final_training_set/perf_analyses_abnormal_5fold_segmentation/', 'dice_weighted_smoothed_lesions')\n",
    "controls_weighted_output_folder = Path('/media/chrisfoulon/HDD2/final_training_set/perf_analyses_controls_5fold_segmentation/', 'dice_weighted_smoothed_lesions')\n",
    "\n",
    "# weight the images of the abnormal dataset\n",
    "abnormal_weighted_dict, abnormal_weighted_df = weight_lesion_dataset(abnormal_df_path, 'segmentation', 'dice_metric', abnormal_weighted_output_folder, 'Unnamed: 0')\n",
    "# save the weighted dataframe\n",
    "abnormal_weighted_df.to_csv(Path('/media/chrisfoulon/HDD2/final_training_set/perf_analyses_abnormal_5fold_segmentation/', 'dice_weighted_smoothed_lesions_segmentation_perf_df.csv'), columns=abnormal_weighted_df.columns)\n",
    "\n",
    "# weight the images of the controls dataset\n",
    "controls_weighted_dict, controls_weighted_df = weight_lesion_dataset(controls_df_path, 'segmentation', 'dice_metric', controls_weighted_output_folder, 'Unnamed: 0')\n",
    "# save the weighted dataframe\n",
    "controls_weighted_df.to_csv(Path('/media/chrisfoulon/HDD2/final_training_set/perf_analyses_controls_5fold_segmentation/', 'dice_weighted_smoothed_lesions_segmentation_perf_df.csv'), columns=controls_weighted_df.columns)\n",
    "\n",
    "\n",
    "# for each row in abnormal_weighted_df, find the corresponding row in controls_weighted_df using 'Unnamed: 0'\n",
    "# then compute the substraction of the nifti images in 'weighted_path' and save the result in a new folder\n",
    "dice_substraction_folder = Path('/media/chrisfoulon/HDD2/final_training_set/', 'dice_subtraction_smoothed_lesions')\n",
    "os.makedirs(dice_substraction_folder, exist_ok=True)\n",
    "for row in tqdm(abnormal_weighted_df.iterrows()):\n",
    "    key = row[1]['Unnamed: 0']\n",
    "    abnormal_path = row[1]['weighted_path']\n",
    "    abnormal_nifti = nib.load(abnormal_path)\n",
    "    abnormal_data = abnormal_nifti.get_fdata()\n",
    "    control_path = controls_weighted_df[controls_weighted_df['Unnamed: 0'] == key]['weighted_path'].values[0]\n",
    "    control_nifti = nib.load(control_path)\n",
    "    control_data = control_nifti.get_fdata()\n",
    "    dice_substraction = abnormal_data - control_data\n",
    "    dice_substraction_nifti = nib.Nifti1Image(dice_substraction, affine=abnormal_nifti.affine)\n",
    "    nib.save(dice_substraction_nifti, Path(dice_substraction_folder, Path(abnormal_path).name.replace('.nii.gz', '.nii')))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T15:29:13.279264078Z",
     "start_time": "2023-07-19T14:21:52.034366812Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
